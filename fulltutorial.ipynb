{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Light Curve Parameter Inference Using LFI -- Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines\n",
    "import os, sys, time, glob\n",
    "import json\n",
    "import copy\n",
    "import scipy\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilby\n",
    "import bilby\n",
    "from bilby.core.prior import Uniform, DeltaFunction\n",
    "from bilby.core.likelihood import GaussianLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nflows\n",
    "from nflows.nn.nets.resnet import ResidualNet\n",
    "from nflows import transforms, distributions, flows\n",
    "from nflows.distributions import StandardNormal\n",
    "from nflows.flows import Flow\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms import CompositeTransform, RandomPermutation\n",
    "import nflows.utils as torchutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extras\n",
    "from IPython.display import clear_output\n",
    "from time import time\n",
    "from time import sleep\n",
    "import corner\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataloading functions\n",
    "\n",
    "# these functions are used to open the data and assign data id's\n",
    "from model.data_processing import open_json, get_names, json_to_df, add_batch_sim_nums_all, get_test_names\n",
    "\n",
    "# these functions are used to ensure the data are the same length (121 points)\n",
    "from model.data_processing import pad_the_data, pad_all_dfs\n",
    "\n",
    "# these functions are used to read in data when it is in csv format\n",
    "from model.data_processing import load_in_data, match_fix_to_var, matched\n",
    "\n",
    "# these functions are used to convert csv files to tensors and create a dataset\n",
    "from model.data_processing import repeated_df_to_tensor, Paper_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing similarity embedding functions\n",
    "\n",
    "from model.embedding import VICRegLoss, ConvResidualBlock, ConvResidualNet, SimilarityEmbedding, train_one_epoch_se, val_one_epoch_se\n",
    "\n",
    "# importing resnet from ML4GW pe\n",
    "\n",
    "from model.resnet import ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing normalizing flow functions\n",
    "\n",
    "from model.normalizingflows import Flow_data, EmbeddingNet, normflow_params, train_one_epoch, val_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing inference functions\n",
    "\n",
    "from model.inference import cast_as_bilby_result, live_plot_samples, ppplot, comparison_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking gpu status, ensures tensors are stored on the same device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Specific Parameters and Priors\n",
    "\n",
    "Light curve generation is done through the nmma package on github: https://github.com/nuclear-multimessenger-astronomy/nmma/tree/main. To properly encode the effects of changing $t$ and $d_L$, each combination of our physical parameters $\\log_{10}(M_{ej})$, $\\log_{10}(V_{ej})$, and $\\log_{10}(X_{lan})$ are repeated 50 times to produce light curves with unique noise instances. A second set of 50 light curves is created by adjusting $t$ and $d_L$ according to the priors shown below. Our training dataset contains 8,729 unique combinations of $\\log_{10}(M_{ej})$, $\\log_{10}(V_{ej})$, and $\\log_{10}(X_{lan})$, resulting in $8,729 \\times 50 \\times 2 = 872,900$ total light curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json specific parameters, adjust this cell with commands when generating light curves -- MANDATORY\n",
    "\n",
    "bands = ['ztfg', 'ztfr', 'ztfi']\n",
    "detection_limit = 22.0\n",
    "num_repeats = 50\n",
    "num_channels = 3\n",
    "num_points = 23\n",
    "in_features = num_points\n",
    "data_dir = '/nobackup/users/mmdesai/new_csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time shift\n",
    "\n",
    "t_zero = 44242.00021937881\n",
    "t_min = 44240.0012975024\n",
    "t_max = 44269.99958898723\n",
    "days = int(round(t_max - t_min))\n",
    "time_step = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors\n",
    "\n",
    "priors = dict()\n",
    "priors['log10_mej'] = Uniform(-1.9, -1, name='log10_mej', latex_label='$\\log(M_{{ej}})$')\n",
    "priors['log10_vej'] = Uniform(-1.52, -0.53, name='log10_vej', latex_label='$\\log(V_{{ej}})$')\n",
    "priors['log10_Xlan'] = Uniform(-9, -3, name='log10_Xlan', latex_label='$\\log(X_{{lan}})$')\n",
    "priors['timeshift'] = Uniform(-2, 6, name='timeshift', latex_label='$\\Delta\\;t$')\n",
    "priors['distance'] = Uniform(50, 200, name='luminosity distance', latex_label='$D$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "This section processes data generated through NMMA. Use this section as a guide for converting the .json files into dataframes. The dataframes are subsequently stored as .csv files. If using the data from Zenodo, skip ahead to the Tensor Processing section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Varied Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path       = '/home/oppenheimer/summer2025/Kilo/data/lc_dir/varied/'\n",
    "detection_limit = 22.0\n",
    "bands           = ['ztfg', 'ztfr', 'ztfi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your existing helper to open a single .json\n",
    "def open_json(file_name, dir_path):\n",
    "    with open(os.path.join(dir_path, file_name)) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# your existing json → DataFrame function\n",
    "def json_to_df(file_name, dir_path, detection_limit, bands):\n",
    "    data = open_json(file_name, dir_path)\n",
    "    df = pd.DataFrame.from_dict(data, orient=\"columns\")\n",
    "    df_unpacked = pd.DataFrame(columns=['t'] + bands)\n",
    "    counter = 0\n",
    "    for j, band in enumerate(bands):\n",
    "        # unpack each band’s [t, value, x]\n",
    "        df_unpacked[['t', band, 'x']] = pd.DataFrame(\n",
    "            df[band].tolist(), index=df.index\n",
    "        )\n",
    "        counter += (df_unpacked[band] != detection_limit).sum()\n",
    "    df_unpacked['num_detections'] = counter\n",
    "    return df_unpacked.drop(columns=['x'])\n",
    "\n",
    "# parameters\n",
    "dir_path       = '/home/oppenheimer/summer2025/Kilo/data/varied/'\n",
    "detection_limit = 22.0\n",
    "bands           = ['ztfg', 'ztfr', 'ztfi']\n",
    "# find all your test_*.json files\n",
    "file_pattern = os.path.join(dir_path, 'test_varied_*.json')\n",
    "all_files    = sorted(glob.glob(file_pattern))  # gives absolute paths\n",
    "\n",
    "# process them all into a list of DataFrames\n",
    "df_list = [\n",
    "    json_to_df(os.path.basename(fp), dir_path, detection_limit, bands)\n",
    "    for fp in all_files\n",
    "]\n",
    "\n",
    "print(len(df_list), \"files found.\")\n",
    "# if you want one big DataFrame\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# now df_all contains the flattened photometry from all 25 000 files\n",
    "print(df_all.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_list[3])\n",
    "\n",
    "#minimum tim across all df_list entries\n",
    "min_time = min([df['t'].min() for df in df_list])\n",
    "#maximum time across all df_list entries\n",
    "max_time = max([df['t'].max() for df in df_list])\n",
    "\n",
    "print(f\"Minimum time: {min_time}, Maximum time: {max_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path       = '/home/oppenheimer/summer2025/Kilo/data/varied/'\n",
    "detection_limit = 22.0\n",
    "bands           = ['ztfg', 'ztfr', 'ztfi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# — your helper functions must already be imported:\n",
    "#    json_to_df(file_name, dir_path, detection_limit, bands)\n",
    "#    pad_all_dfs(df_list, t_min, t_max, step, data_filler, bands)\n",
    "\n",
    "# 0) USER PARAMETERS\n",
    "# dir_path        = \"/path/to/your/jsons\"\n",
    "detection_limit = 22.0\n",
    "bands           = ['ztfg', 'ztfr', 'ztfi']\n",
    "chunk_size      = 5000        # how many sims to process per batch\n",
    "batch_size      = 50          # sims per batch for batch_id\n",
    "output_csv      = \"all_lightcurves.csv\"\n",
    "\n",
    "# 1) GATHER ALL JSON PATHS\n",
    "file_list = sorted(glob.glob(os.path.join(dir_path, \"test_*.json\")))\n",
    "\n",
    "# 2) DETERMINE GLOBAL TIME GRID\n",
    "raw_min, raw_max = np.inf, -np.inf\n",
    "for fp in file_list:\n",
    "    df = json_to_df(os.path.basename(fp), dir_path, detection_limit, bands)\n",
    "    raw_min = min(raw_min, df['t'].min())\n",
    "    raw_max = max(raw_max, df['t'].max())\n",
    "\n",
    "step  = 1.0\n",
    "t_min = np.floor(raw_min)\n",
    "t_max = np.ceil(raw_max) + step\n",
    "\n",
    "# 3) REMOVE OLD CSV\n",
    "if os.path.exists(output_csv):\n",
    "    os.remove(output_csv)\n",
    "\n",
    "# 4) PROCESS IN CHUNKS\n",
    "for start in range(0, len(file_list), chunk_size):\n",
    "    chunk_files = file_list[start : start + chunk_size]\n",
    "    df_list     = []\n",
    "\n",
    "    # 4a) LOAD & ANNOTATE\n",
    "    for sim_idx, fp in enumerate(chunk_files, start=start):\n",
    "        df = json_to_df(os.path.basename(fp), dir_path, detection_limit, bands)\n",
    "        df['sim_id']         = sim_idx\n",
    "        df['num_detections'] = (df[bands] < detection_limit).sum().sum()\n",
    "        df_list.append(df)\n",
    "\n",
    "    # 4b) PAD TO UNIFORM LENGTH\n",
    "    padded = pad_all_dfs(df_list, t_min, t_max, step, detection_limit, bands)\n",
    "\n",
    "    # 4c) ASSIGN batch_id\n",
    "    for idx, df in enumerate(padded, start=start):\n",
    "        df['batch_id'] = idx // batch_size\n",
    "\n",
    "    # 4d) CONCAT & APPEND TO CSV\n",
    "    chunk_df = pd.concat(padded, ignore_index=True)\n",
    "    chunk_df.to_csv(\n",
    "        output_csv,\n",
    "        mode='a',\n",
    "        index=False,\n",
    "        header=not os.path.exists(output_csv)\n",
    "    )\n",
    "\n",
    "    # 4e) CLEAN UP\n",
    "    del df_list, padded, chunk_df\n",
    "    gc.collect()\n",
    "\n",
    "# 5) (optional) READ BACK FULL DATAFRAME\n",
    "df_all = pd.read_csv(output_csv)\n",
    "print(\"Final table shape:\", df_all.shape)\n",
    "print(\"Time spans:\", df_all['t'].min(), \"→\", df_all['t'].max())\n",
    "print(\"Unique batch_ids:\", sorted(df_all['batch_id'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_list[3])\n",
    "# print(padded_list[3])\n",
    "\n",
    "#print df_all with a particular sim_id\n",
    "sim_id = 10  # change this to the sim_id you want to inspect\n",
    "df_sim = df_all[df_all['sim_id'] == sim_id]\n",
    "print(f\"Data for sim_id {sim_id}:\\n\", df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_simembed_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the varied data\n",
    "\n",
    "sim_path = '/nobackup/users/mmdesai/lowcsimdata'\n",
    "num_sims = 25000\n",
    "\n",
    "simembed_num_lc_list = [24750, 25000, 25000, 25000, 25000, 25000, 24850, 25000, 25000, 25000]\n",
    "\n",
    "for i in range (0, 10):\n",
    "    # get the names of each file\n",
    "    file_names = get_names(sim_path, 'varied', i, simembed_num_lc_list[i])\n",
    "    # open the files as dataframes\n",
    "    varied_simembed_dict['varied_simembed_data_{}'.format(i)] = json_to_df(file_names, simembed_num_lc_list[i])\n",
    "    # pad the data\n",
    "    varied_simembed_dict['varied_simembed_data_{}'.format(i)] = pad_all_dfs(varied_simembed_dict['varied_simembed_data_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_simembed_dict['varied_simembed_data_0'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a small sample of the varied light curves\n",
    "\n",
    "for i in range(0, 300, 50):\n",
    "    plt.scatter(varied_simembed_dict['varied_simembed_data_0'][i]['t'], varied_simembed_dict['varied_simembed_data_0'][i]['ztfg'], color = 'g')\n",
    "    plt.scatter(varied_simembed_dict['varied_simembed_data_0'][i]['t'], varied_simembed_dict['varied_simembed_data_0'][i]['ztfr'], color = 'r')\n",
    "    plt.scatter(varied_simembed_dict['varied_simembed_data_0'][i]['t'], varied_simembed_dict['varied_simembed_data_0'][i]['ztfi'], color = 'c')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inj_path_simembed = '/home/oppenheimer/summer2025/Kilo/data/varied/'\n",
    "\n",
    "varied_inj_df = pd.DataFrame()\n",
    "varied_params = open_json('injection_varied.json', inj_path_simembed)\n",
    "varied_inj_df['mej'] = varied_params['injections']['content']['log10_mej']\n",
    "varied_inj_df['vej'] = varied_params['injections']['content']['log10_vej']\n",
    "varied_inj_df['xlan'] = varied_params['injections']['content']['log10_Xlan']\n",
    "varied_inj_df['shift'] = varied_params['injections']['content']['timeshift']\n",
    "varied_inj_df['distance'] = varied_params['injections']['content']['luminosity_distance']\n",
    "varied_inj_df['sim_id'] = varied_params['injections']['content']['simulation_id']\n",
    "varied_injections = varied_inj_df\n",
    "\n",
    "print(varied_injections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your varied_injections DataFrame calls its sim key something else, rename it:\n",
    "# varied_injections = varied_injections.rename(columns={ 'simulation_id': 'sim_id' })\n",
    "\n",
    "# select only the columns you care about from injections\n",
    "inj_cols = ['sim_id', 'mej','vej','xlan','shift','distance']\n",
    "\n",
    "# merge onto your full light‐curve table\n",
    "df_final = df_all.merge(\n",
    "    varied_injections[inj_cols],\n",
    "    on='sim_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Final shape:\", df_final.shape)  # should be (~3 300 000, original_cols+5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a particular sim_id\n",
    "sim_id = 10  # change this to the sim_id you want to inspect\n",
    "df_sim = df_final[df_final['sim_id'] == sim_id]\n",
    "print(f\"Data for sim_id {sim_id}:\\n\", df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list = []\n",
    "for i, df in enumerate(padded_list):\n",
    "    inj = varied_injections.iloc[i]  # a Series with index ['mej','vej',…,'sim_id']\n",
    "    # drop the sim_id from inj if you don’t want to re–assign it\n",
    "    for col in ['mej','vej','xlan','shift','distance']:\n",
    "        df[col] = inj[col]\n",
    "    merged_list.append(df)\n",
    "\n",
    "# finally, glue them all together\n",
    "df_final = pd.concat(merged_list, ignore_index=True)\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print df_final with a particular sim_id\n",
    "print(df_final[df_final['sim_id'] == 24999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# injection files for the additional data\n",
    "\n",
    "inj_path_simembed = '/nobackup/users/mmdesai/final_injections'\n",
    "varied_injections = {}\n",
    "\n",
    "for i in range(0, 10):\n",
    "    varied_inj_df = pd.DataFrame()\n",
    "    varied_params = open_json('/injection_simembed_varied_{}.json'.format(i), inj_path_simembed)\n",
    "    varied_inj_df['mej'] = varied_params['injections']['content']['log10_mej']\n",
    "    varied_inj_df['vej'] = varied_params['injections']['content']['log10_vej']\n",
    "    varied_inj_df['xlan'] = varied_params['injections']['content']['log10_Xlan']\n",
    "    varied_inj_df['shift'] = varied_params['injections']['content']['timeshift']\n",
    "    varied_inj_df['distance'] = varied_params['injections']['content']['luminosity_distance']\n",
    "    varied_inj_df['sim_id'] = varied_params['injections']['content']['simulation_id']\n",
    "    varied_injections['varied_inj_df{}'.format(i)] = varied_inj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_injections['varied_inj_df0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframe lists\n",
    "\n",
    "all_varied_data_list = [0] * 10\n",
    "\n",
    "for i in range(0, 10):\n",
    "    all_varied_data_list[i] = pd.concat(varied_simembed_dict['varied_simembed_data_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with injection parameters\n",
    "\n",
    "all_varied_datawparams_list = [0] * 10\n",
    "\n",
    "for i in range(0, 10):\n",
    "    all_varied_datawparams_list[i] = all_varied_data_list[i].merge(varied_injections['varied_inj_df{}'.format(i)], on = 'sim_id')\n",
    "    # save as csv file\n",
    "    all_varied_datawparams_list[i].to_csv('/nobackup/users/mmdesai/final_csv/varied_lowc_{}.csv'.format(i), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_varied_datawparams_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fixed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path= '/home/oppenheimer/summer2025/Kilo/data/fixed/'\n",
    "detection_limit = 22.0\n",
    "bands= ['ztfg', 'ztfr', 'ztfi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "\n",
    "# --- 1) load your dataframes as before ---\n",
    "file_list = sorted(glob.glob(os.path.join(dir_path, 'test_*.json')))\n",
    "\n",
    "df_fixed_list = []\n",
    "for sim_idx, fp in enumerate(file_list):\n",
    "    # assume you have a single-file json_to_df → DataFrame\n",
    "    df = json_to_df(os.path.basename(fp), dir_path, detection_limit, bands)\n",
    "\n",
    "    # add sim_id\n",
    "    df['sim_id'] = sim_idx\n",
    "\n",
    "    # count detections across all bands\n",
    "    # (this matches your old logic: count all values != detection_limit)\n",
    "    detections = (df[bands] < detection_limit).sum().sum()\n",
    "    df['num_detections'] = detections\n",
    "\n",
    "    df_fixed_list.append(df)\n",
    "\n",
    "# raw_min = min(df['t'].min() for df in df_list)\n",
    "# raw_max = max(df['t'].max() for df in df_list)\n",
    "\n",
    "step  = 1.0\n",
    "t_min = np.floor(raw_min)            # round down\n",
    "t_max = np.ceil(raw_max) + step      # round up, then add one step\n",
    "\n",
    "padded_fixed_list = pad_all_dfs(\n",
    "    df_fixed_list,\n",
    "    t_min,\n",
    "    t_max,\n",
    "    step,\n",
    "    data_filler=np.nan,\n",
    "    bands=bands\n",
    ")\n",
    "batch_size = 50  # adjust as needed\n",
    "\n",
    "for sim_idx, df in enumerate(padded_fixed_list):\n",
    "    # integer division gives you 0 for sims 0–49, 1 for sims 50–99, etc.\n",
    "    df['batch_id'] = sim_idx // batch_size\n",
    "\n",
    "df_fixed_all = pd.concat(padded_fixed_list, ignore_index=True)\n",
    "print(\"time runs from\", df_all['t'].min(), \"to\", df_all['t'].max())\n",
    "print(\"should equal\", t_min, \"→\", t_max-step)\n",
    "print(\"total rows:\", len(df_fixed_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# — your helper functions must already be imported:\n",
    "#    json_to_df(file_name, dir_path, detection_limit, bands)\n",
    "#    pad_all_dfs(df_list, t_min, t_max, step, data_filler, bands)\n",
    "\n",
    "# 0) USER PARAMETERS\n",
    "# dir_path        = \"/path/to/your/jsons\"\n",
    "detection_limit = 22.0\n",
    "bands           = ['ztfg', 'ztfr', 'ztfi']\n",
    "chunk_size      = 5000        # how many sims to process per batch\n",
    "batch_size      = 50          # sims per batch for batch_id\n",
    "output_csv      = \"all_lightcurves.csv\"\n",
    "\n",
    "# 1) GATHER ALL JSON PATHS\n",
    "file_list = sorted(glob.glob(os.path.join(dir_path, \"test_*.json\")))\n",
    "\n",
    "# 2) DETERMINE GLOBAL TIME GRID\n",
    "raw_min, raw_max = np.inf, -np.inf\n",
    "for fp in file_list:\n",
    "    df = json_to_df(os.path.basename(fp), dir_path, detection_limit, bands)\n",
    "    raw_min = min(raw_min, df['t'].min())\n",
    "    raw_max = max(raw_max, df['t'].max())\n",
    "\n",
    "step  = 1.0\n",
    "# t_min = np.floor(raw_min)\n",
    "# t_max = np.ceil(raw_max) + step\n",
    "\n",
    "# 3) REMOVE OLD CSV\n",
    "if os.path.exists(output_csv):\n",
    "    os.remove(output_csv)\n",
    "\n",
    "# 4) PROCESS IN CHUNKS\n",
    "for start in range(0, len(file_list), chunk_size):\n",
    "    chunk_files = file_list[start : start + chunk_size]\n",
    "    df_list     = []\n",
    "\n",
    "    # 4a) LOAD & ANNOTATE\n",
    "    for sim_idx, fp in enumerate(chunk_files, start=start):\n",
    "        df = json_to_df(os.path.basename(fp), dir_path, detection_limit, bands)\n",
    "        df['sim_id']         = sim_idx\n",
    "        df['num_detections'] = (df[bands] < detection_limit).sum().sum()\n",
    "        df_list.append(df)\n",
    "\n",
    "    # 4b) PAD TO UNIFORM LENGTH\n",
    "    padded = pad_all_dfs(df_list, t_min, t_max, step, detection_limit, bands)\n",
    "\n",
    "    # 4c) ASSIGN batch_id\n",
    "    for idx, df in enumerate(padded, start=start):\n",
    "        df['batch_id'] = idx // batch_size\n",
    "\n",
    "    # 4d) CONCAT & APPEND TO CSV\n",
    "    chunk_df = pd.concat(padded, ignore_index=True)\n",
    "    chunk_df.to_csv(\n",
    "        output_csv,\n",
    "        mode='a',\n",
    "        index=False,\n",
    "        header=not os.path.exists(output_csv)\n",
    "    )\n",
    "\n",
    "    # 4e) CLEAN UP\n",
    "    del df_list, padded, chunk_df\n",
    "    gc.collect()\n",
    "\n",
    "# 5) (optional) READ BACK FULL DATAFRAME\n",
    "df_fixed_all = pd.read_csv(output_csv)\n",
    "print(\"Final table shape:\", df_fixed_all.shape)\n",
    "print(\"Time spans:\", df_fixed_all['t'].min(), \"→\", df_fixed_all['t'].max())\n",
    "print(\"Unique batch_ids:\", sorted(df_fixed_all['batch_id'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(padded_fixed_list[5])\n",
    "# print(df_fixed_list[5])\n",
    "\n",
    "# print df_fixed_all with a particular sim_id\n",
    "sim_id = 10  # change this to the sim_id you want to inspect\n",
    "df_sim = df_fixed_all[df_fixed_all['sim_id'] == sim_id]\n",
    "print(f\"Data for sim_id {sim_id}:\\n\", df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inj_path_simembed = '/home/oppenheimer/summer2025/Kilo/data/fixed/'\n",
    "\n",
    "fixed_inj_df = pd.DataFrame()\n",
    "fixed_params = open_json('injection_fixed.json', inj_path_simembed)\n",
    "fixed_inj_df['mej'] = fixed_params['injections']['content']['log10_mej']\n",
    "fixed_inj_df['vej'] = fixed_params['injections']['content']['log10_vej']\n",
    "fixed_inj_df['xlan'] = fixed_params['injections']['content']['log10_Xlan']\n",
    "fixed_inj_df['shift'] = fixed_params['injections']['content']['timeshift']\n",
    "fixed_inj_df['distance'] = fixed_params['injections']['content']['luminosity_distance']\n",
    "fixed_inj_df['sim_id'] = fixed_params['injections']['content']['simulation_id']\n",
    "fixed_injections = fixed_inj_df\n",
    "\n",
    "print(fixed_injections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your varied_injections DataFrame calls its sim key something else, rename it:\n",
    "# varied_injections = varied_injections.rename(columns={ 'simulation_id': 'sim_id' })\n",
    "\n",
    "# select only the columns you care about from injections\n",
    "inj_cols = ['sim_id', 'mej','vej','xlan','shift','distance']\n",
    "\n",
    "# merge onto your full light‐curve table\n",
    "df_fixed_final = df_fixed_all.merge(\n",
    "    fixed_injections[inj_cols],\n",
    "    on='sim_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Final shape:\", df_fixed_final.shape)  # should be (~3 300 000, original_cols+5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_fixed_list = []\n",
    "for i, df in enumerate(padded_fixed_list):\n",
    "    inj = fixed_injections.iloc[i]  # a Series with index ['mej','vej',…,'sim_id']\n",
    "    # drop the sim_id from inj if you don’t want to re–assign it\n",
    "    for col in ['mej','vej','xlan','shift','distance']:\n",
    "        df[col] = inj[col]\n",
    "    merged_fixed_list.append(df)\n",
    "\n",
    "# finally, glue them all together\n",
    "df_fixed_final = pd.concat(merged_fixed_list, ignore_index=True)\n",
    "print(df_fixed_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print df_fixed_final with a particular sim_id\n",
    "print(df_fixed_final[df_fixed_final['sim_id'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_simembed_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fixed data\n",
    "\n",
    "sim_path = '/nobackup/users/mmdesai/lowcsimdata'\n",
    "num_sims = 25000\n",
    "\n",
    "simembed_num_lc_list = [24900, 25000, 25000, 25000, 25000, 25000, 25000, 24800, 25000, 25000]\n",
    "\n",
    "for i in range(0, 10):\n",
    "    # get the names of each file\n",
    "    file_names = get_names(sim_path, 'fixed', i, simembed_num_lc_list[i])\n",
    "    # open the files as dataframes\n",
    "    fixed_simembed_dict['fixed_simembed_data_{}'.format(i)] = json_to_df(file_names, simembed_num_lc_list[i])\n",
    "    # pad the data\n",
    "    fixed_simembed_dict['fixed_simembed_data_{}'.format(i)] = pad_all_dfs(fixed_simembed_dict['fixed_simembed_data_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_simembed_dict['fixed_simembed_data_0'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 300, 50):\n",
    "    plt.scatter(fixed_simembed_dict['fixed_simembed_data_0'][i]['t'], fixed_simembed_dict['fixed_simembed_data_0'][i]['ztfg'], color = 'g')\n",
    "    plt.scatter(fixed_simembed_dict['fixed_simembed_data_0'][i]['t'], fixed_simembed_dict['fixed_simembed_data_0'][i]['ztfr'], color = 'r')\n",
    "    plt.scatter(fixed_simembed_dict['fixed_simembed_data_0'][i]['t'], fixed_simembed_dict['fixed_simembed_data_0'][i]['ztfi'], color = 'c')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot lightcurves from padded_fixed_list\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_n = 3\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(first_n):\n",
    "    df = padded_fixed_list[i]\n",
    "    bands = ['ztfg']\n",
    "    for band in bands:\n",
    "        plt.scatter(df['t'], df[band], label=f\"{band} (sim {i})\")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Flux\")\n",
    "plt.title(f\"First {first_n} Light Curves Overlapped in All Three Bands\")\n",
    "plt.legend(loc=\"upper right\", ncol=first_n)  # adjust layout if it’s crowded\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot lightcurves from padded_fixed_list\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_n = 3\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(first_n):\n",
    "    df = padded_fixed_list[i]\n",
    "    bands = ['ztfr']\n",
    "    for band in bands:\n",
    "        plt.scatter(df['t'], df[band], label=f\"{band} (sim {i})\")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Flux\")\n",
    "plt.title(f\"First {first_n} Light Curves Overlapped in All Three Bands\")\n",
    "plt.legend(loc=\"upper right\", ncol=first_n)  # adjust layout if it’s crowded\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot lightcurves from padded_fixed_list\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_n = 3\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(first_n):\n",
    "    df = padded_fixed_list[i]\n",
    "    bands = ['ztfi']\n",
    "    for band in bands:\n",
    "        plt.scatter(df['t'], df[band], label=f\"{band} (sim {i})\")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Flux\")\n",
    "plt.title(f\"First {first_n} Light Curves Overlapped in All Three Bands\")\n",
    "plt.legend(loc=\"upper right\", ncol=first_n)  # adjust layout if it’s crowded\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# injection files\n",
    "\n",
    "inj_path_simembed = '/nobackup/users/mmdesai/final_injections'\n",
    "fixed_injections = {}\n",
    "\n",
    "for i in range(0, 10):\n",
    "    fixed_inj_df = pd.DataFrame()\n",
    "    fixed_params = open_json('/injection_simembed_fixed_{}.json'.format(i), inj_path_simembed)\n",
    "    fixed_inj_df['mej'] = fixed_params['injections']['content']['log10_mej']\n",
    "    fixed_inj_df['vej'] = fixed_params['injections']['content']['log10_vej']\n",
    "    fixed_inj_df['xlan'] = fixed_params['injections']['content']['log10_Xlan']\n",
    "    fixed_inj_df['shift'] = fixed_params['injections']['content']['timeshift']\n",
    "    fixed_inj_df['distance'] = fixed_params['injections']['content']['luminosity_distance']\n",
    "    fixed_inj_df['sim_id'] = fixed_params['injections']['content']['simulation_id']\n",
    "    fixed_injections['fixed_inj_df{}'.format(i)] = fixed_inj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_injections['fixed_inj_df0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframe lists\n",
    "\n",
    "all_fixed_data_list = [0] * 10\n",
    "\n",
    "for i in range(0, 10):\n",
    "    all_fixed_data_list[i] = pd.concat(fixed_simembed_dict['fixed_simembed_data_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with injection parameters\n",
    "\n",
    "all_fixed_datawparams_list = [0] * 10\n",
    "\n",
    "for i in range(0, 10):\n",
    "    all_fixed_datawparams_list[i] = all_fixed_data_list[i].merge(fixed_injections['fixed_inj_df{}'.format(i)], on = 'sim_id')\n",
    "    # save as csv file\n",
    "    all_fixed_datawparams_list[i].to_csv('/nobackup/users/mmdesai/final_csv/fixed_lowc_{}.csv'.format(i), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fixed_datawparams_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load in Data\n",
    "\n",
    "If the data is stored as a .csv file, use this section to further process the data and import it to the notebook. This section ensures that the fixed (unshifted) and shifted light curves are properly paired and assigns each light curve a unique simulation id (sim_id). Batch numbers are also added, with 50 light curves in a batch. Each set of 50 light curves have the same mass, velocity, and lanthanide fraction of the ejecta. The fixed ones peak at the same time and are set to a luminosity distance of 50 Mpc, while the 50 repeated shifted light curves have a time and distance generated from a uniform prior. We also set the condition for number of detections in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the csv files are stored\n",
    "\n",
    "data_dir = '/nobackup/users/mmdesai/final_csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the minimum number of detections needed\n",
    "\n",
    "min_num_detections = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## First Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df1 = matched(data_dir, 'varied', 'fixed', 0, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_batch_sim_nums_all(matched_df1)\n",
    "matched_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list1 = []\n",
    "for i in range(int(len(matched_df1)/num_points/50)):\n",
    "    batch_df = matched_df1.loc[matched_df1['batch_id'] == i]\n",
    "    if batch_df['num_detections_x'].min() >= min_num_detections:\n",
    "        true_list1.append(batch_df)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_df1 = pd.concat(true_list1)\n",
    "detected_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varied\n",
    "\n",
    "var_df = detected_df1.iloc[:, :12]\n",
    "var_df.columns = var_df.columns.str.rstrip('_x')\n",
    "var_df = var_df.drop(columns=['key_1'])\n",
    "add_batch_sim_nums_all(var_df)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed\n",
    "\n",
    "fix_df = detected_df1.iloc[:, 12:]\n",
    "fix_df.columns = fix_df.columns.str.rstrip('_y')\n",
    "add_batch_sim_nums_all(fix_df)\n",
    "fix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Second Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df2 = matched(data_dir, 'varied', 'fixed', 10, 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_batch_sim_nums_all(matched_df2)\n",
    "new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list2 = []\n",
    "for i in range(int(len(matched_df2)/121/50)):\n",
    "    batch_df = matched_df2.loc[matched_df2['batch_id'] == i]\n",
    "    if batch_df['num_detections_x'].min() >= min_num_detections:\n",
    "        true_list2.append(batch_df)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_df2 = pd.concat(true_list2)\n",
    "detected_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varied\n",
    "\n",
    "var_df = detected_df2.iloc[:, :12]\n",
    "var_df.columns = var_df.columns.str.rstrip('_x')\n",
    "var_df = var_df.drop(columns=['key_1'])\n",
    "add_batch_sim_nums_all(var_df)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed\n",
    "\n",
    "fix_df = detected_df2.iloc[:, 12:]\n",
    "fix_df.columns = fix_df.columns.str.rstrip('_y')\n",
    "add_batch_sim_nums_all(fix_df)\n",
    "fix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_order = ['t', 'ztfg', 'ztfr', 'ztfi', 'num_detections', 'mej', 'vej', 'xlan', 'shift', 'distance', 'batch_id', 'sim_id']\n",
    "df_varied = df_final[desired_order]\n",
    "df_fixed = df_fixed_final[desired_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_varied\n",
    "# print df_varied with a particular sim_id\n",
    "sim_id = 0  # change this to the sim_id you want to inspect\n",
    "df_sim = df_varied[df_varied['sim_id'] == sim_id]\n",
    "print(f\"Data for sim_id {sim_id}:\\n\", df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fixed\n",
    "# print df_varied with a particular sim_id\n",
    "sim_id = 0  # change this to the sim_id you want to inspect\n",
    "df_sim = df_varied[df_varied['sim_id'] == sim_id]\n",
    "print(f\"Data for sim_id {sim_id}:\\n\", df_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_df_to_tensor(df_varied, df_fixed, batches):\n",
    "    '''\n",
    "    Converts dataframes into pytorch tensors\n",
    "    Inputs:\n",
    "        df_varied: dataframe containing the shifted light curve information\n",
    "        df_fixed: dataframe containing the analagous fixed light curve information\n",
    "        batches: number of unique mass, velocity, and lanthanide injections\n",
    "    Outputs:\n",
    "        data_shifted_list: list of tensors of shape [repeats, channels, num_points] containing the shifted light curve photometry\n",
    "        data_unshifted_list: list of tensors of shape [repeats, channels, num_points] containing the fixed light curve photometry\n",
    "        param_shifted_list: list of tensors of shape [repeats, 1, 5] containing the injection parameters of the shifted light curves\n",
    "        param_unshifted_list: list of tensors of shape [repeats, 1, 5] containing the injection parameters of the fixed light curves\n",
    "    '''\n",
    "    data_shifted_list = []\n",
    "    data_unshifted_list = []\n",
    "    param_shifted_list = []\n",
    "    param_unshifted_list = []\n",
    "    for idx in tqdm(range(0, batches)):\n",
    "        data_shifted = torch.tensor(df_varied.loc[df_varied['batch_id'] == idx].iloc[:, 1:4].values.reshape(num_repeats, num_points, num_channels), \n",
    "                                    dtype=torch.float32).transpose(1, 2)\n",
    "        data_unshifted = torch.tensor(df_fixed.loc[df_fixed['batch_id'] == idx].iloc[:, 1:4].values.reshape(num_repeats, num_points, num_channels), \n",
    "                                    dtype=torch.float32).transpose(1, 2)\n",
    "        param_shifted = torch.tensor(df_varied.loc[df_varied['batch_id'] == idx].iloc[::num_points, 5:10].values, \n",
    "                                    dtype=torch.float32).unsqueeze(2).transpose(1,2)\n",
    "        param_unshifted = torch.tensor(df_fixed.loc[df_fixed['batch_id'] == idx].iloc[::num_points, 5:10].values, \n",
    "                                    dtype=torch.float32).unsqueeze(2).transpose(1,2)\n",
    "        data_shifted_list.append(data_shifted)\n",
    "        data_unshifted_list.append(data_unshifted)\n",
    "        param_shifted_list.append(param_shifted)\n",
    "        param_unshifted_list.append(param_unshifted)\n",
    "    return data_shifted_list, data_unshifted_list, param_shifted_list, param_unshifted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the data to tensors on gpu -- ONLY RUN IF YOU ARE USING DATA FROM A CSV FILE\n",
    "num_repeats = 50\n",
    "num_channels = 3\n",
    "num_points = 33\n",
    "num_batches_paper_sample = len(df_varied['batch_id'].unique())\n",
    "print(f\"Number of batches in the paper sample: {num_batches_paper_sample}\")\n",
    "data_shifted_paper, data_unshifted_paper, param_shifted_paper, param_unshifted_paper = repeated_df_to_tensor(\n",
    "    df_varied, df_fixed, num_batches_paper_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_shifted_paper[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the path to the tensors from Zenodo and load the data in\n",
    "\n",
    "data_shifted_paper1 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_shifted_paper4.pt')\n",
    "data_unshifted_paper1 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_unshifted_paper4.pt')\n",
    "param_shifted_paper1 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_shifted_paper4.pt')\n",
    "param_unshifted_paper1 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_unshifted_paper4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shifted_paper2 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_shifted_paper5.pt')\n",
    "data_unshifted_paper2 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_unshifted_paper5.pt')\n",
    "param_shifted_paper2 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_shifted_paper5.pt')\n",
    "param_unshifted_paper2 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_unshifted_paper5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shifted_paper = torch.stack(data_shifted_paper1 + data_shifted_paper2)\n",
    "data_unshifted_paper = torch.stack(data_unshifted_paper1 + data_unshifted_paper2)\n",
    "param_shifted_paper = torch.stack(param_shifted_paper1 + param_shifted_paper2)\n",
    "param_unshifted_paper = torch.stack(param_unshifted_paper1 + param_unshifted_paper2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is organized into number of repeats (50) x number of channels (3 - ztfg, ztfr, ztfi) x number of points (121)\n",
    "\n",
    "data_shifted_paper[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parameters stored are in the order: mass, velocity, lanthanide fraction, time, and distance\n",
    "\n",
    "param_shifted_paper[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of batches, each batch contains 50 light curves\n",
    "\n",
    "num_batches_paper_sample = len(data_shifted_paper)\n",
    "print(num_batches_paper_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Embedding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss\n",
    "vicreg_loss = VICRegLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "\n",
    "dataset_paper = Paper_data(data_shifted_paper, data_unshifted_paper, param_shifted_paper, param_unshifted_paper, num_batches_paper_sample)\n",
    "\n",
    "# check the dataset shape\n",
    "_, t, d, _ = dataset_paper[4]\n",
    "_.shape, t.shape, d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training, testing, and validation\n",
    "\n",
    "num_batches_paper_sample = len(data_shifted_paper)\n",
    "\n",
    "train_set_size_paper = int(0.8 * num_batches_paper_sample)    \n",
    "val_set_size_paper = int(0.1 * num_batches_paper_sample)     \n",
    "test_set_size_paper = num_batches_paper_sample - train_set_size_paper - val_set_size_paper\n",
    "\n",
    "print(f\"Train set size: {train_set_size_paper}, Validation set size: {val_set_size_paper}, Test set size: {test_set_size_paper}\")\n",
    "\n",
    "train_data_paper, val_data_paper, test_data_paper = torch.utils.data.random_split(\n",
    "    dataset_paper, [train_set_size_paper, val_set_size_paper, test_set_size_paper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and shuffle the data\n",
    "\n",
    "train_data_loader_paper = DataLoader(train_data_paper, batch_size=50, shuffle=True)\n",
    "val_data_loader_paper = DataLoader(val_data_paper, batch_size=50, shuffle=True)\n",
    "test_data_loader_paper = DataLoader(test_data_paper, batch_size=1, shuffle=False)\n",
    "\n",
    "# check lengths\n",
    "len(train_data_loader_paper), len(test_data_loader_paper), len(val_data_loader_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "Taking a look at some of the data distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "Checking if the selected data still is uniform after selecting data with greater than 8 detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mej_list = []\n",
    "vej_list = []\n",
    "xlan_list = []\n",
    "\n",
    "for i in range(len(param_shifted_paper)):\n",
    "    mej = param_shifted_paper[i][0][0][0]\n",
    "    vej = param_shifted_paper[i][0][0][1]\n",
    "    xlan = param_shifted_paper[i][0][0][2]\n",
    "    mej_list.append(mej)\n",
    "    vej_list.append(vej)\n",
    "    xlan_list.append(xlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(mej_list, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mej_list = []\n",
    "vej_list = []\n",
    "xlan_list = []\n",
    "\n",
    "for i in range(len(param_shifted_paper)):\n",
    "    mej = param_shifted_paper[i][0][0][0]\n",
    "    vej = param_shifted_paper[i][0][0][1]\n",
    "    xlan = param_shifted_paper[i][0][0][2]\n",
    "    mej_list.append(mej)\n",
    "    vej_list.append(vej)\n",
    "    xlan_list.append(xlan)\n",
    "\n",
    "hist = plt.hist(mej_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(vej_list, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(vej_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ist = plt.hist(xlan_list, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(xlan_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_list = []\n",
    "shift_list = []\n",
    "\n",
    "for i in range(len(param_shifted_paper)):\n",
    "    for j in range(0, 50):\n",
    "        dist = param_shifted_paper[i][j][0][4]\n",
    "        shift = param_shifted_paper[i][j][0][3]\n",
    "        dist_list.append(dist)\n",
    "        shift_list.append(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_shifted_paper[0][20][0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(dist_list, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(dist_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shifted_paper[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(shift_list, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(shift_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Light Curve Graphs\n",
    "\n",
    "Some plotting codes for visualizing the light curves from the .csv files -- SKIP FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_colors = ['seagreen', 'crimson', 'blue']\n",
    "varied_colors = ['mediumaquamarine', 'salmon', 'skyblue']\n",
    "label_list = ['g band', 'r band', 'i band']\n",
    "bands = ['ztfg', 'ztfr', 'ztfi']\n",
    "\n",
    "def varied_fixed_plot(varied_df, fixed_df, sim_id, xlim_min=None, xlim_max=None, title = False, bands=bands):\n",
    "    varied_data = varied_df.loc[varied_df['sim_id'] == sim_id]\n",
    "    fixed_data = fixed_df.loc[fixed_df['sim_id'] == sim_id]\n",
    "    if 'mej' in varied_data.columns:\n",
    "        mej = varied_data.iloc[0, varied_data.columns.get_loc('mej')]\n",
    "    if 'vej' in varied_data.columns:\n",
    "        vej = varied_data.iloc[0, varied_data.columns.get_loc('vej')]\n",
    "    if 'xlan' in varied_data.columns:\n",
    "        xlan = varied_data.iloc[0, varied_data.columns.get_loc('xlan')]\n",
    "    fig, axs = plt.subplots(3, sharex=True, sharey=True, figsize=(7,7))\n",
    "    for i in range(len(bands)):\n",
    "        axs[i].scatter(fixed_data['t'], fixed_data[bands[i]], label = 'fixed, ' + label_list[i] , color = fixed_colors[i], s = 10)\n",
    "        axs[i].scatter(varied_data['t'], varied_data[bands[i]], label = 'shifted, ' + label_list[i], color = varied_colors[i], s = 10)\n",
    "        plt.gca().invert_yaxis()\n",
    "        axs[i].legend()\n",
    "        if (xlim_min != None) & (xlim_max != None):\n",
    "            plt.xlim(xlim_min, xlim_max)\n",
    "    fig.supxlabel('Time (Days)')\n",
    "    fig.supylabel('Magnitude')\n",
    "    if title == True:\n",
    "        if ('vej' in varied_data.columns) and ('mej' in varied_data.columns):\n",
    "            fig.suptitle('Light Curve for $\\log_{{10}}(M_{{ej}})$: {:.2f}, $\\log_{{10}}(V_{{ej}})$: {:.2f}, \\n$\\log_{{10}}(X_{{lan}})$: {:.2f}'.format(mej, vej, xlan), \n",
    "                         fontsize = 15)\n",
    "    else:\n",
    "        pass\n",
    "    if 'shift' in varied_data.columns:\n",
    "        shift = varied_data.iloc[0, varied_data.columns.get_loc('shift')]\n",
    "    if 'distance' in varied_data.columns:\n",
    "        distance = varied_data.iloc[0, varied_data.columns.get_loc('distance')]\n",
    "    print(shift, distance)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_fixed_plot(df_varied, df_fixed, 2,title = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts the neural network on the gpu\n",
    "similarity_embedding = SimilarityEmbedding(num_dim=7, num_hidden_layers_f=1, num_hidden_layers_h=1, num_blocks=4, kernel_size=5, num_dim_final=5).to(device)\n",
    "num_dim = 7\n",
    "\n",
    "# optimizes\n",
    "optimizer = optim.Adam(similarity_embedding.parameters(), lr=2.747064325271709e-05)\n",
    "\n",
    "# sets learning rate steps\n",
    "scheduler_1 = optim.lr_scheduler.ConstantLR(optimizer, total_iters=5) #constant lr\n",
    "scheduler_2 = optim.lr_scheduler.OneCycleLR(optimizer, total_steps=20, max_lr=2e-3) #one cycle - increase and then decrease\n",
    "scheduler_3 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler_1, scheduler_2, scheduler_3], milestones=[5, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data is the right shape for similarity embedding\n",
    "\n",
    "for var_inj_se, fix_inj_se, var_data_se, fix_data_se in train_data_loader_paper:\n",
    "    var_inj_se = var_inj_se.reshape((-1,)+var_inj_se.shape[2:])\n",
    "    fix_inj_se = fix_inj_se.reshape((-1,)+fix_inj_se.shape[2:])\n",
    "    var_data_se = var_data_se.reshape((-1,)+var_data_se.shape[2:])\n",
    "    fix_data_se = fix_data_se.reshape((-1,)+fix_data_se.shape[2:])\n",
    "    break\n",
    "var_inj_se.shape, var_data_se.shape, fix_inj_se.shape, fix_data_se.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shapes\n",
    "\n",
    "embed, rep = similarity_embedding(var_data_se)\n",
    "embed.shape, rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the data and calculate the loss for one example to check for bugs\n",
    "\n",
    "emb_aug, rep_aug = similarity_embedding(var_data_se)\n",
    "emb_orig, rep_orig = similarity_embedding(fix_data_se)\n",
    "vicreg_loss(emb_aug, emb_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print neural network parameters that require gradients and sum parameters\n",
    "\n",
    "sum_param=0\n",
    "for name, param in similarity_embedding.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param.numel())\n",
    "        sum_param+=param.numel()\n",
    "print(sum_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to tensorboard for data visualization\n",
    "\n",
    "writer = SummaryWriter(\"torchlogs/\")\n",
    "model = similarity_embedding\n",
    "writer.add_graph(model, var_data_se)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training the neural network for many epochs\n",
    "\n",
    "epoch_number = 0\n",
    "EPOCHS = 50\n",
    "\n",
    "sim_val_loss = []\n",
    "sim_train_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # set the vicreg weights\n",
    "    wt_repr, wt_cov, wt_std = (1, 1, 1)\n",
    "    print(f\"VicReg wts: {wt_repr} {wt_cov} {wt_std}\")\n",
    "    \n",
    "    # Gradient tracking\n",
    "    similarity_embedding.train(True)\n",
    "    avg_train_loss = train_one_epoch_se(epoch_number, writer, train_data_loader_paper,\n",
    "                                        similarity_embedding, optimizer, vicreg_loss, verbose=True,\n",
    "                                        wt_repr=wt_repr, wt_cov=wt_cov, wt_std=wt_std)\n",
    "    sim_train_loss.append(avg_train_loss)\n",
    "    \n",
    "    # no gradient tracking, for validation\n",
    "    similarity_embedding.train(False)\n",
    "    similarity_embedding.eval()\n",
    "    avg_val_loss = val_one_epoch_se(epoch_number, writer, val_data_loader_paper,\n",
    "                                    similarity_embedding, vicreg_loss,\n",
    "                                    wt_repr=wt_repr, wt_cov=wt_cov, wt_std=wt_std)\n",
    "    sim_val_loss.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Train/Val Sim Loss after epoch: {avg_train_loss:.4f}/{avg_val_loss:.4f}\")\n",
    "\n",
    "    epoch_number += 1\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train/val loss \n",
    "\n",
    "epoch_list = range(0,len(sim_train_loss))\n",
    "plt.plot(epoch_list, sim_train_loss, label = 'Similarity Embedding (Train)', color = 'royalblue', alpha = 0.8, marker = 's')\n",
    "plt.plot(epoch_list, sim_val_loss, label = 'Similarity Embedding (Val)', color = 'lightsteelblue', alpha=0.8, linestyle=\"dashed\")\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedded weights (input your save path)\n",
    "\n",
    "SAVEPATH = '/home/oppenheimer/summer2025/Kilo/weights/similarity-embedding-weights-tutorial2.pth'\n",
    "torch.save(similarity_embedding.state_dict(), SAVEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Embedding Weights\n",
    "\n",
    "If you do not want to retrain the embedding, just load the pretrained weights here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights \n",
    "\n",
    "similarity_embedding = SimilarityEmbedding(num_dim=7, num_hidden_layers_f=1, num_hidden_layers_h=1, num_blocks=4, kernel_size=5, num_dim_final=5).to(device)\n",
    "num_dim = 7\n",
    "\n",
    "SAVEPATH = '/home/oppenheimer/summer2025/Kilo/weights/similarity-embedding-weights-tutorial2.pth'\n",
    "similarity_embedding.load_state_dict(torch.load(SAVEPATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the embedding for all test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_embedding.train(False)\n",
    "data_loader = test_data_loader_paper\n",
    "\n",
    "similarity_outputs_1 = []\n",
    "\n",
    "for idx, (_, shift_test, data_test, data_test_orig) in enumerate(data_loader):\n",
    "    _ = _.reshape((-1,)+_.shape[2:])\n",
    "    data_test = data_test.reshape((-1,)+data_test.shape[2:])\n",
    "    data_test_orig = data_test_orig.reshape((-1,)+data_test_orig.shape[2:])\n",
    "    shift_test = shift_test.reshape((-1,)+shift_test.shape[2:])\n",
    "    if not ((shift_test[0][0][0] < -1) and (shift_test[0][0][0] > -1.25)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][1] < -0.5) and (shift_test[0][0][1] > -0.75)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][2] < -3) and (shift_test[0][0][2] > -4.5)):\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        _, similarity_output = similarity_embedding(data_test)\n",
    "    similarity_outputs_1.append(similarity_output)\n",
    "\n",
    "similarity_outputs_2 = []\n",
    "\n",
    "for idx, (_, shift_test, data_test, data_test_orig) in enumerate(data_loader):\n",
    "    _ = _.reshape((-1,)+_.shape[2:])\n",
    "    data_test = data_test.reshape((-1,)+data_test.shape[2:])\n",
    "    data_test_orig = data_test_orig.reshape((-1,)+data_test_orig.shape[2:])\n",
    "    shift_test = shift_test.reshape((-1,)+shift_test.shape[2:])\n",
    "    if not ((shift_test[0][0][0] < -1.25) and (shift_test[0][0][0] > -1.5)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][1] < -0.75) and (shift_test[0][0][1] > -1)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][2] < -4.5) and (shift_test[0][0][2] > -6)):\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        _, similarity_output = similarity_embedding(data_test)\n",
    "    similarity_outputs_2.append(similarity_output)\n",
    "\n",
    "similarity_outputs_3 = []\n",
    "\n",
    "for idx, (_, shift_test, data_test, data_test_orig) in enumerate(data_loader):\n",
    "    _ = _.reshape((-1,)+_.shape[2:])\n",
    "    data_test = data_test.reshape((-1,)+data_test.shape[2:])\n",
    "    data_test_orig = data_test_orig.reshape((-1,)+data_test_orig.shape[2:])\n",
    "    shift_test = shift_test.reshape((-1,)+shift_test.shape[2:])\n",
    "    if not ((shift_test[0][0][0] < -1.5) and (shift_test[0][0][0] > -1.75)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][1] < -1) and (shift_test[0][0][1] > -1.25)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][2] < -6) and (shift_test[0][0][2] > -7.5)):\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        _, similarity_output = similarity_embedding(data_test)\n",
    "    similarity_outputs_3.append(similarity_output)\n",
    "\n",
    "similarity_outputs_4 = []\n",
    "\n",
    "for idx, (_, shift_test, data_test, data_test_orig) in enumerate(data_loader):\n",
    "    _ = _.reshape((-1,)+_.shape[2:])\n",
    "    data_test = data_test.reshape((-1,)+data_test.shape[2:])\n",
    "    data_test_orig = data_test_orig.reshape((-1,)+data_test_orig.shape[2:])\n",
    "    shift_test = shift_test.reshape((-1,)+shift_test.shape[2:])\n",
    "    if not ((shift_test[0][0][0] < -1.75) and (shift_test[0][0][0] > -2.5)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][1] < -1.25) and (shift_test[0][0][1] > -1.55)):\n",
    "        continue\n",
    "    if not ((shift_test[0][0][2] < -7.5) and (shift_test[0][0][2] > -9)):\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        _, similarity_output = similarity_embedding(data_test)\n",
    "    similarity_outputs_4.append(similarity_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_outputs_1 = torch.stack(similarity_outputs_1)\n",
    "similarity_outputs_2 = torch.stack(similarity_outputs_2)\n",
    "similarity_outputs_3 = torch.stack(similarity_outputs_3)\n",
    "similarity_outputs_4 = torch.stack(similarity_outputs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_outputs_1.shape, similarity_outputs_2.shape, similarity_outputs_3.shape, similarity_outputs_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = corner.corner(\n",
    "    similarity_outputs_1.cpu().numpy().reshape((similarity_outputs_1.shape[0]*similarity_outputs_1.shape[1], num_dim)),\n",
    "    quantiles=[0.16, 0.5, 0.84], color=\"C1\"#, range = [[-1.2,-0.8], [-1.2,-0.8], [-1.2,-0.8]]\n",
    ")\n",
    "c1_line = mlines.Line2D([], [], color='C1', \n",
    "                            label='-1 > $\\log_{{10}}(M_{{ej}})$ > -1.25, -0.5 > $\\log_{{10}}(V_{{ej}})$ > -0.75, -3.0 > $\\log_{{10}}(X_{{lan}})$ > -4.5')\n",
    "\n",
    "figure = corner.corner(\n",
    "    similarity_outputs_2.cpu().numpy().reshape((similarity_outputs_2.shape[0]*similarity_outputs_2.shape[1], num_dim)),\n",
    "    quantiles=[0.16, 0.5, 0.84], fig=figure, \n",
    "    color=\"C2\", # range = [[-1.2,-0.8], [-1.2,-0.8], [-1.2,-0.8]]\n",
    ")\n",
    "c2_line = mlines.Line2D([], [], color='C2', \n",
    "                            label='-1.25 > $\\log_{{10}}(M_{{ej}})$ > -1.5, -0.75 > $\\log_{{10}}(V_{{ej}})$ > -1.0, -4.5 > $\\log_{{10}}(X_{{lan}})$ > -6.0')\n",
    "\n",
    "figure = corner.corner(\n",
    "    similarity_outputs_3.cpu().numpy().reshape((similarity_outputs_3.shape[0]*similarity_outputs_3.shape[1], num_dim)),\n",
    "    quantiles=[0.16, 0.5, 0.84], fig=figure, color=\"C3\"\n",
    ")\n",
    "c3_line = mlines.Line2D([], [], color='C3', \n",
    "                            label='-1.5 > $\\log_{{10}}(M_{{ej}})$ > -1.75, -1.0 > $\\log_{{10}}(V_{{ej}})$ > -1.25, -6.0 > $\\log_{{10}}(X_{{lan}})$ > -7.5')\n",
    "\n",
    "figure = corner.corner(\n",
    "    similarity_outputs_4.cpu().numpy().reshape((similarity_outputs_4.shape[0]*similarity_outputs_4.shape[1], num_dim)),\n",
    "    quantiles=[0.16, 0.5, 0.84], fig=figure, color=\"C4\"\n",
    ")\n",
    "c4_line = mlines.Line2D([], [], color='C4', \n",
    "                            label='-1.75 > $\\log_{{10}}(M_{{ej}})$ > -1.9, -1.25 > $\\log_{{10}}(V_{{ej}})$ > -1.53, -7.5 > $\\log_{{10}}(X_{{lan}})$ > -9.0')\n",
    "\n",
    "plt.legend(handles=\n",
    "           [c1_line, c2_line, c3_line, c4_line],\n",
    "           bbox_to_anchor=(0.3, 7.3),\n",
    "           fontsize = 18\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Normalizing Flow Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Varied Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_normflow_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the varied data\n",
    "\n",
    "norm_path = '/nobackup/users/mmdesai/lowcflowdata'\n",
    "num_sims = 25000\n",
    "\n",
    "normflow_num_lc_list = [25000, 25000, 25000, 24843, 25000, 24917, 25000, 25000, 24706, 25000]\n",
    "\n",
    "for i in range(3, 10):\n",
    "    # get the names of each file\n",
    "    file_names = get_names(norm_path, 'varied', i, normflow_num_lc_list[i])\n",
    "    # open the files as dataframes\n",
    "    varied_normflow_dict['varied_normflow_data_{}'.format(i)] = json_to_df(file_names, normflow_num_lc_list[i])\n",
    "    # pad the data\n",
    "    varied_normflow_dict['varied_normflow_data_{}'.format(i)] = pad_all_dfs(varied_normflow_dict['varied_normflow_data_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_normflow_dict['varied_normflow_data_0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 50):\n",
    "    plt.scatter(varied_normflow_dict['varied_normflow_data_0'][i]['t'], varied_normflow_dict['varied_normflow_data_0'][i]['ztfg'], color = 'g')\n",
    "    plt.scatter(varied_normflow_dict['varied_normflow_data_0'][i]['t'], varied_normflow_dict['varied_normflow_data_0'][i]['ztfr'], color = 'r')\n",
    "    plt.scatter(varied_normflow_dict['varied_normflow_data_0'][i]['t'], varied_normflow_dict['varied_normflow_data_0'][i]['ztfi'], color = 'c')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# injection files\n",
    "\n",
    "varied_normflow_inj_dict = {}\n",
    "inj_path_normflow = '/nobackup/users/mmdesai/final_injections/'\n",
    "\n",
    "for i in range(0, 10):\n",
    "    varied_normflow_inj_dict['varied_inj_df{}'.format(i)] = pd.DataFrame()\n",
    "    varied_params = open_json('injection_normflow_varied_{}.json'.format(i), inj_path_normflow)\n",
    "    varied_normflow_inj_dict['varied_inj_df{}'.format(i)]['mej'] = varied_params['injections']['content']['log10_mej']\n",
    "    varied_normflow_inj_dict['varied_inj_df{}'.format(i)]['vej'] = varied_params['injections']['content']['log10_vej']\n",
    "    varied_normflow_inj_dict['varied_inj_df{}'.format(i)]['xlan'] = varied_params['injections']['content']['log10_Xlan']\n",
    "    varied_normflow_inj_dict['varied_inj_df{}'.format(i)]['shift'] = varied_params['injections']['content']['timeshift']\n",
    "    varied_normflow_inj_dict['varied_inj_df{}'.format(i)]['distance'] = varied_params['injections']['content']['luminosity_distance']\n",
    "    varied_normflow_inj_dict['varied_inj_df{}'.format(i)]['sim_id'] = varied_params['injections']['content']['simulation_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_normflow_inj_dict['varied_inj_df0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframe lists\n",
    "\n",
    "all_varied_data_list_flow = [0] * 10\n",
    "\n",
    "for i in range(0, 10):\n",
    "    all_varied_data_list_flow[i] = pd.concat(varied_normflow_dict['varied_normflow_data_{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with injection parameters\n",
    "\n",
    "all_varied_datawparams_list_flow = [0] * 10\n",
    "\n",
    "for i in range(0, 10):\n",
    "    all_varied_datawparams_list_flow[i] = all_varied_data_list_flow[i].merge(varied_normflow_inj_dict['varied_inj_df{}'.format(i)], on = 'sim_id')\n",
    "    # save as csv file\n",
    "    all_varied_datawparams_list_flow[i].to_csv('/nobackup/users/mmdesai/final_csv/flow_varied_lowc_{}.csv'.format(i), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_varied_datawparams_list_flow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load in the Data\n",
    "\n",
    "ONLY CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_flow = '/nobackup/users/mmdesai/final_csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow1 = load_in_data(data_dir_flow, 'flow_varied', 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_df1 = df_flow1.loc[df_flow1['num_detections'] >= 20]\n",
    "detected_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varied\n",
    "\n",
    "detected_df1 = detected_df1.iloc[:29275950, :12]\n",
    "add_batch_sim_nums_all(detected_df1)\n",
    "detected_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df_flow2 = matched(data_dir_flow, 'flow_varied', 'flow_fixed', 10, 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_flow2 = matched_df_flow2.loc[matched_df_flow2['mej_x'] >= -1.9].copy()\n",
    "new_df_flow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_df2 = new_df_flow2.loc[new_df_flow2['num_detections_x'] >= 8]\n",
    "detected_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varied\n",
    "\n",
    "var_df = detected_df2.iloc[:27097950, :12]\n",
    "var_df.columns = var_df.columns.str.rstrip('_x')\n",
    "var_df = var_df.drop(columns=['key_1'])\n",
    "add_batch_sim_nums_all(var_df)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed\n",
    "\n",
    "fix_df = detected_df2.iloc[:27097950, 12:]\n",
    "fix_df.columns = fix_df.columns.str.rstrip('_y')\n",
    "add_batch_sim_nums_all(fix_df)\n",
    "fix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df_flow3 = matched(data_dir_flow, 'flow_varied', 'flow_fixed', 20, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_flow3 = matched_df_flow3.loc[matched_df_flow3['mej_x'] >= -1.9].copy()\n",
    "new_df_flow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_df3 = new_df_flow3.loc[new_df_flow3['num_detections_x'] >= 8]\n",
    "detected_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varied\n",
    "\n",
    "var_df = detected_df3.iloc[:27073750, :12]\n",
    "var_df.columns = var_df.columns.str.rstrip('_x')\n",
    "var_df = var_df.drop(columns=['key_1'])\n",
    "add_batch_sim_nums_all(var_df)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed\n",
    "\n",
    "fix_df = detected_df3.iloc[:27073750, 12:]\n",
    "fix_df.columns = fix_df.columns.str.rstrip('_y')\n",
    "add_batch_sim_nums_all(fix_df)\n",
    "fix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var_df['mej'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var_df['vej'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var_df['xlan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var_df['distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the data from csv to tensors on gpu -- Don't run if tensors are already stored and available\n",
    "\n",
    "num_lc_flow = len(detected_df1['batch_id'].unique())\n",
    "data_shifted_flow, param_shifted_flow = test_df_to_tensor(detected_df1, num_lc_flow, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shifted_flow1 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_shifted_flow4.pt')\n",
    "data_unshifted_flow1 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_unshifted_flow4.pt')\n",
    "param_shifted_flow1 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_shifted_flow4.pt')\n",
    "param_unshifted_flow1 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_unshifted_flow4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shifted_flow2 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_shifted_flow7.pt')\n",
    "data_unshifted_flow2 = torch.load('/nobackup/users/mmdesai/updated_tensors/data_unshifted_flow7.pt')\n",
    "param_shifted_flow2 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_shifted_flow7.pt')\n",
    "param_unshifted_flow2 = torch.load('/nobackup/users/mmdesai/updated_tensors/param_unshifted_flow7.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shifted_flow = torch.stack(data_shifted_flow1 + data_shifted_flow2)\n",
    "param_shifted_flow = torch.stack(param_shifted_flow1 + param_shifted_flow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lc_flow = len(data_shifted_flow)\n",
    "print(num_lc_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shifted_flow[0].shape, param_shifted_flow[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normflow = Flow_data(data_shifted_flow, param_shifted_flow, num_lc_flow)\n",
    "\n",
    "# check the dataset shape\n",
    "t, d = dataset_normflow[4]\n",
    "t.shape, d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training, testing, and validation\n",
    "\n",
    "train_set_size_flow = int(0.8 * num_lc_flow)    \n",
    "val_set_size_flow = int(0.1 * num_lc_flow)     \n",
    "test_set_size_flow = num_lc_flow - train_set_size_flow - val_set_size_flow\n",
    "\n",
    "train_data_flow, val_data_flow, test_data_flow = torch.utils.data.random_split(\n",
    "    dataset_normflow, [train_set_size_flow, val_set_size_flow, test_set_size_flow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and shuffle the data\n",
    "\n",
    "train_data_loader_flow = DataLoader(train_data_flow, batch_size=25, shuffle=True)\n",
    "val_data_loader_flow = DataLoader(val_data_flow, batch_size=25, shuffle=True)\n",
    "test_data_loader_flow = DataLoader(test_data_flow, batch_size=1, shuffle=False)\n",
    "\n",
    "# check lengths\n",
    "len(train_data_loader_flow), len(test_data_loader_flow), len(val_data_loader_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first instance of data\n",
    "\n",
    "for var_inj, var_data in train_data_loader_flow:\n",
    "    var_inj = var_inj.reshape((-1,)+var_inj.shape[2:])\n",
    "    var_data = var_data.reshape((-1,)+var_data.shape[2:])\n",
    "\n",
    "    break\n",
    "var_inj.shape, var_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mej_list = []\n",
    "vej_list = []\n",
    "xlan_list = []\n",
    "\n",
    "for i in range(len(param_shifted_flow)):\n",
    "    mej = param_shifted_flow[i][0][0][0]\n",
    "    vej = param_shifted_flow[i][0][0][1]\n",
    "    xlan = param_shifted_flow[i][0][0][2]\n",
    "    mej_list.append(mej)\n",
    "    vej_list.append(vej)\n",
    "    xlan_list.append(xlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(mej_list, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(vej_list, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(xlan_list, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_list = []\n",
    "shift_list = []\n",
    "\n",
    "for i in range(len(param_shifted_flow)):\n",
    "    for j in range(0, 50):\n",
    "        dist = param_shifted_flow[i][j][0][4]\n",
    "        shift = param_shifted_flow[i][j][0][3]\n",
    "        dist_list.append(dist)\n",
    "        shift_list.append(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(dist_list, bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(shift_list, bins = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially Freeze the Similarity Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_inj_se, var_data_se in train_data_loader_flow:\n",
    "    var_inj_se = var_inj_se.reshape((-1,)+var_inj_se.shape[2:]).to(device)\n",
    "    var_data_se = var_data_se.reshape((-1,)+var_data_se.shape[2:]).to(device)\n",
    "    break\n",
    "\n",
    "# check shapes\n",
    "print(var_data_se.shape, var_inj_se.shape)\n",
    "_, rep = similarity_embedding(var_data_se)  # _.shape = batch_size x 1 x 10, # rep.shape = batch_size x 1 x 2\n",
    "print(_.shape, rep.shape)\n",
    "context_features = rep.shape[-1]\n",
    "print('number of context_features: ', context_features)\n",
    "print('number of dimensions: ', num_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "\n",
    "transform, base_dist, embedding_net = normflow_params(similarity_embedding, 9, 5, 90, context_features=context_features, num_dim=num_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = Flow(transform, base_dist, embedding_net).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of trainable parameters: ', sum(p.numel() for p in flow.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(train_data_loader_flow, 1):\n",
    "    augmented_shift, augmented_data = val\n",
    "    augmented_shift = augmented_shift[...,0:3].to(device)\n",
    "    augmented_shift = augmented_shift.flatten(0, 2).to(device)\n",
    "    augmented_data = augmented_data.reshape(-1, 3, num_points).to(device)\n",
    "    print(augmented_shift.shape, augmented_data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_embedding(augmented_data)[0].shape, similarity_embedding(augmented_data)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_loss = -flow.log_prob(augmented_shift, context=augmented_data).mean()\n",
    "flow_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.SGD(flow.parameters(), lr=0.0000912, momentum=0.5)\n",
    "# scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# UNCOMMENT AND RUN TO TRAIN FROM SCRATCH\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "EPOCHS = 50\n",
    "epoch_number = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    flow.train(True)\n",
    "    for name, param in flow._embedding_net.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    avg_train_loss = train_one_epoch(epoch_number, writer, train_data_loader_flow, flow, optimizer, 2)\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    flow.train(False)\n",
    "    avg_val_loss = val_one_epoch(epoch_number, writer, val_data_loader_flow, flow, 2)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    print(f\"Train/Val flow Loss after epoch: {avg_train_loss:.4f}/{avg_val_loss:.4f}\")\n",
    "    epoch_number += 1\n",
    "    scheduler.step(avg_val_loss)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Current LR = {:.3e}\".format(param_group['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalizing flow weights\n",
    "\n",
    "PATH_nflow = '/nobackup/users/mmdesai/flow_weights_tutorial.pth'\n",
    "torch.save(flow.state_dict(), PATH_nflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the normalizing flow weights\n",
    "\n",
    "context_features = 7\n",
    "transform, base_dist, embedding_net = normflow_params(similarity_embedding, 9, 5, 90, context_features=context_features, num_dim=num_dim) \n",
    "flow = Flow(transform, base_dist, embedding_net).to(device=device)\n",
    "\n",
    "PATH_nflow = '/nobackup/users/mmdesai/flow_weights_tutorial.pth'\n",
    "flow.load_state_dict(torch.load(PATH_nflow, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the comparision of the train/val loss for the three scenarios\n",
    "\n",
    "epoch_list = range(0,200)\n",
    "\n",
    "plt.plot(epoch_list[:len(train_loss_list)], train_loss_list[:len(train_loss_list)], label = 'Current Run', color = 'k')\n",
    "plt.plot(epoch_list[:len(val_loss_list)], val_loss_list[:len(val_loss_list)], label = 'Validation', color = 'k', linestyle = 'dashed')\n",
    "plt.ylabel('- Log. Prob.')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (shift_test, data_test) in enumerate(test_data_loader_flow):\n",
    "    data_test = data_test.reshape((-1,)+data_test.shape[2:])\n",
    "    shift_test = shift_test.reshape((-1,)+shift_test.shape[2:])\n",
    "    if idx % 100 !=0: continue \n",
    "    with torch.no_grad():\n",
    "        samples = flow.sample(1000, context=data_test[0].reshape((1, 3, num_points)))\n",
    "    live_plot_samples(samples.cpu().reshape(1000,3), shift_test[0][0].cpu()[...,0:3])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
